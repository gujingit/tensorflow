#coding=utf-8
import tensorflow as tf
import numpy as np
import input_data

mnist = input_data.read_data_sets("MNIST_data/",one_hot=True)

x = tf.placeholder(tf.float32,[None,784])
#x = tf.add((x-tf.reduce_min(x))/(tf.reduce_max(x)-tf.reduce_min(x)),0.00001)
#with tf.Session() as sess:
#	print(sess.run(x[1,:],feed_dict={x:mnist.test.images}))
y = tf.placeholder(tf.float32,[None,10])
w0 = tf.Variable(tf.random_normal([784,800]))
b0 = tf.Variable(tf.zeros([800]))
w1 = tf.Variable(tf.random_normal([800,10]))
b1 = tf.Variable(tf.zeros([10]))
#w2 = tf.Variable(tf.random_normal([784,10]))
#b2 = tf.Variable(tf.zeros([10]))

#=====train=====
l1 = tf.nn.relu(tf.matmul(x,w0)+b0)
l2 = tf.nn.relu(tf.matmul(l1,w1)+b1)
#l3 = tf.nn.relu(tf.matmul(l2,w2)+b2)
output = l2

#loss = tf.reduce_mean(tf.reduce_sum(tf.square(output-y)))

loss = -tf.reduce_sum(y*tf.log(output))
#train = tf.train.AdamOptimizer(0.1).minimize(loss)
train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

for i in range(1000):
	batch_xs,batch_ys = mnist.train.next_batch(100)
	t,c=sess.run([train,loss],feed_dict={x:batch_xs,y:batch_ys})
	if i%100 ==0:
		correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(y,1))
		accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
		print("train",sess.run(accuracy,feed_dict={x:batch_xs,y:batch_ys}))
		print("cost:",c)
		

#evaluation
correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
print(sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels}))

